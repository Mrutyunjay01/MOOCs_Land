{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZnhyvN0Xiwt"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self , input):\n",
    "        return input.view(input.size(0) , -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def __init__(self , channel , height , width):\n",
    "        super(Unflatten , self).__init__()\n",
    "        \n",
    "        self.channel = channel\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "          \n",
    "    def forward(self , input):\n",
    "        return input.view(input.size(0) , self.channel , self.height , self.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLciLLgDXqgZ"
   },
   "outputs": [],
   "source": [
    "class convVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self , latent_size):\n",
    "        super(convVAE , self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "             \n",
    "            nn.Conv2d(3 , 32 , 4 , 4 , 1) ,\n",
    "            nn.BatchNorm2d(32) ,\n",
    "            nn.ReLU() , \n",
    "            \n",
    "            nn.Conv2d(32 , 64 , 4 , 2 , 1) , \n",
    "            nn.BatchNorm2d(64) ,\n",
    "            nn.ReLU() , \n",
    "            \n",
    "            nn.Conv2d(64 , 128 , 4 , 2 , 1) , \n",
    "            nn.BatchNorm2d(128) , \n",
    "            nn.ReLU() , \n",
    "            \n",
    "            Flatten() , \n",
    "            nn.Linear(2048 , 1024) , \n",
    "            nn.ReLU()            \n",
    "            )\n",
    "        \n",
    "        self.mu = nn.Linear(1024 , self.latent_size)\n",
    "        self.logvar = nn.Linear(1024 , self.latent_size)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_size , 1024) , \n",
    "            nn.ReLU() , \n",
    "            nn.Linear(1024 , 2048) , \n",
    "            nn.ReLU() , \n",
    "            Unflatten(128 , 4 , 4) , \n",
    "            nn.ConvTranspose2d(128 , 64 , 4 , 2 , 1) , \n",
    "            nn.BatchNorm2d(64) , \n",
    "            nn.ReLU() , \n",
    "            \n",
    "            nn.ConvTranspose2d(64 , 32 , 4 , 2 , 1) , \n",
    "            nn.BatchNorm2d(32) ,\n",
    "            nn.ReLU() , \n",
    "            \n",
    "            nn.ConvTranspose2d(32 , 16 , 4 , 2 , 1) , \n",
    "            nn.BatchNorm2d(16) , \n",
    "            nn.ReLU() , \n",
    "            nn.ConvTranspose2d(16 , 3 , 4 , 2 , 1) , \n",
    "            nn.BatchNorm2d(3) , \n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "    def encode(self , x):\n",
    "        h = self.encoder(x)\n",
    "        mu , logvar = self.mu(h) , self.logvar(h)\n",
    "        return mu , logvar            \n",
    "        \n",
    "    def reparameterize(self , mu , logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu +eps * std\n",
    "        return z\n",
    "        \n",
    "    def decode(self , z):\n",
    "        decoded_z = self.decoder(z)\n",
    "        return decoded_z\n",
    "        \n",
    "    def forward(self , x):\n",
    "        mu , logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu , logvar)\n",
    "        return self.decode(z) , mu , logvar\n",
    "            \n",
    "vae = convVAE(1024)\n",
    "vae.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_VBs7sgXuo1"
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x , x , mu , logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x , x , reduction = \"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
    "    return torch.mean(BCE + KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZEnKXqcXxou"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(vae.parameters() , lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqhC50qOX0He"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_list.append(epoch)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in tqdm(image_loader):\n",
    "              #with torch.autograd.set_detect_anomaly(True):\n",
    "            input_img = i.to(\"cuda\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output , mu , logvar = vae.forward(input_img)\n",
    "\n",
    "              #loss  = dice_loss(output , ground_truth )\n",
    "            loss = loss_function(output , input_img , mu , logvar)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() \n",
    "            #print(loss)\n",
    "            input_img.to(\"cpu\")\n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "       \n",
    "    train_loss = train_loss / 23000\n",
    "    loss_list.append(train_loss)\n",
    "    print(epoch , train_loss)\n",
    "torch.save(vae.state_dict() , r\"C:\\Users\\Abhrant\\Desktop\\abhrant\\work\\DEEP_LEARNING\\vae_weight.pt\")\n",
    "plt.plot(epoch_list , loss_list)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwDeLixsmcSOn+9NmryH0s",
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
